\section{Introduction}
\begin{figure}
    \centering
    \tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=8em, text centered, rounded corners, minimum height=4em]

    \tikzstyle{line} = [draw, -latex, color=blue]

    \begin{tikzpicture}[node distance = 6em, auto, thick]
        \node [block] (Agent) {\textbf{Agent}};
        \node [block, below of=Agent] (Environment) {\textbf{Environment}};
        
        \path [line] (Agent.0) --++ (4em,0em) |- node [near start]{Action $a_t$} (Environment.0);
        \path [line] (Environment.190) --++ (-6em,0em) |- node [near start] {New state  $s_{t+1}$} (Agent.170);
        \path [line] (Environment.170) --++ (-4.25em,0em) |- node [near start, right] {Reward $r_{t+1}$} (Agent.190);
    \end{tikzpicture}
    \caption{The MDP Framework: As illustrated in the figure, the agent, which represents an AI algorithm, and the environment, an abstract entity delivering uncertain outcomes, engage in a time-sequenced, iterative interaction. Credit: Adapted from Sutton \& Barto, 2018 \cite{RL} }
    \label{fig:my_label}
\end{figure}

\subsection{What is Reinforcement Learning?}
Reinforcement Learning (RL) is a subfield of artificial intelligence that centers around an \vocab{agent} learning to make optimal decisions by interacting with its \vocab{environment}, receiving feedback in the form of \vocab{rewards} or penalties, and adjusting its behavior accordingly to maximize cumulative rewards over time \cite{RL}. The agent is  an entity that observes the environment, takes actions based on these observations, and learns from the results to improve its decisions over time. The environment is the dynamic context or `world' within which the agent operates. At each time step, the agent perceives a possibly partial or noisy state of the environment, referred to as an \vocab{observation}. Based on it's (possibly partial) observation of the state of the world, the agent decides an \textit{action} to take. The environment changes when the agent acts on it. However, the environment may also evolve independently.  Based on the action it took, the agent acquires a scalar \textit{reward}, signaling the desirability of its current state.  The primary objective of the agent is to maximize this cumulative reward, known as the \vocab{return}. Reinforcement learning methods equip the agent with strategies to learn behaviours that contribute towards achieving this goal.

\subsection{Connections to Machine Learning }
At a high level, machine learning is about using algorithms to learn from data, and then making predictions or decisions about new unseen data by extrapolating learned insights. Most of modern machine learning focuses on learning \textit{functions} from data. In essence, a function takes an input (or set of inputs), performs some operations, and produces an output. In machine learning, this function is learned from the data such that it can predict the output (target variable) given a new input. 
Machine learning follows a straightforward methodology: select a versatile function approximator such as a deep neural network, identify a suitable loss function, and employ gradient descent for optimizing the network's parameters. The overall learning problem in machine learning boils down to an optimization problem: find the parameters that minimize the loss function. By adjusting the parameters to achieve this, the neural network learns the function that best maps inputs to outputs given the data.

In reinforcement learning (RL), the reduction from learning problem to an optimization problem presents more complexities compared to supervised learning. A primary challenge lies in the fact that the function we aim to optimize - the agent's expected total reward - isn't entirely accessible for analytic operations. This is primarily due to its dependence on two unknown elements: the dynamics model, which predicts the future state of the system based on current state and actions, and the reward function, which defines the desirability of each state. These two components are usually unknown in RL, making the optimization process more challenging. Another challenge arises from the fact that the agent's input data is significantly influenced by its own behavior. This dependency makes it difficult to design algorithms that guarantee consistent improvements, as changes in the agent's behavior can affect the data it uses for learning. Further complicating the matter is the existence of multiple potential functions that one might aim to approximate, which introduces an additional layer of complexity. We will delve into these various approximation targets in more detail in Section...

\subsection{History of Deep Reinforcement Learning}
Deep reinforcement learning combines the power of reinforcement learning with the versatility of neural networks for function approximation. The fusion of reinforcement learning and neural networks has a history stretching back to the early 1990s, when Tesauroâ€™s TD-Gammon, a backgammon playing AI equipped with a neural network value function, performed at a level comparable to top human players. Since then, neural networks have found consistent use in system identification and control.

Lin's 1993 thesis furthered the exploration of this amalgamation, implementing various reinforcement learning algorithms in conjunction with neural networks in a robotics context.

Despite these promising beginnings, reinforcement learning with nonlinear function approximation remained relatively obscure for about two decades. The majority of reinforcement learning studies presented at leading machine learning conferences like NIPS and ICML were largely concentrated on theoretical results or toy problems that utilized linear or tabular function approximators.

A significant shift occurred in the early 2010s when deep learning started demonstrating extraordinary empirical success, particularly in fields like speech recognition and computer vision. Most conventional reinforcement learning methods, tailored to linear or tabular functions, fell short when it came to learning functions that require multi-step computation. In contrast, deep neural networks could effectively approximate these complex functions, with their performance in supervised learning scenarios providing evidence of the tractability of their optimization.

The field of deep reinforcement learning saw a resurgence of interest following the ground-breaking results by Mnih et al. Their work showcased an AI learning to play a range of Atari games using screen images as inputs, and a variant of Q-learning for policy control. Their approach outperformed previous methods using evolutionary algorithms, despite employing a more challenging input representation.

Following this resurgence, numerous intriguing results emerged. Notably, Silver et al. demonstrated an AI system that could play the game of Go better than human experts, using a blend of supervised learning and several reinforcement learning steps for training deep neural networks, coupled with a tree search algorithm.

\subsection{Structure of Reinforcement Learning Algorithms}
Reinforcement Learning (RL) algorithms, while diverse in their approaches, share a common structural anatomy that allows for a broad high-level understanding of their operation. Essentially, these algorithms can be decomposed into three fundamental components, which are present, to varying degrees of complexity, in most, if not all, RL methodologies.
\begin{enumerate}
    \item \textbf{Sample Generation}: This is the phase where the agent interacts with the environment by executing its current policy. The aim is to collect experience samples, consisting of states, actions, and rewards. These samples serve as the empirical basis for the agent's learning process. The way these samples are generated can depend on various strategies such as purely exploratory methods, exploitation of current knowledge, or a combination of both (exploration-exploitation tradeoff).

    \item \textbf{Model Fitting/Return Estimation}: Tthe agent leverages the collected samples to either construct a model of the environment or directly estimate the expected return (cumulative future reward) for various state-action pairs. These estimates are pivotal in guiding the agent's future actions. While model-based methods strive to understand the environment's dynamics, model-free methods circumvent this step by trying to directly learn the optimal policy or value function.

    \item \textbf{Policy Improvement}: Based on the information gained from the previous steps, the agent refines its policy to better navigate the environment. The improvement could be a deterministic change, like in Policy Iteration, or a probabilistic one as in Policy Gradient methods. The refined policy is expected to guide the agent towards actions that maximize the cumulative reward.
\end{enumerate}
These components form a loop that continues until the agent's policy converges towards an optimal policy, or a predefined stopping criterion is met. It's important to note that while these components are a simplification, the intricacies and nuances of different RL algorithms can add significant complexity to each of these steps.
\begin{figure}
    \centering
    \tikzstyle{block} = [rectangle, draw=black, fill=RedViolet!5!gray!5, text centered, rounded corners, minimum height=3em, text width=6em, font=\footnotesize]
    \tikzset{line/.style={draw, -latex, line width=0.2mm, crimson}}

    \begin{tikzpicture}[node distance = 9em, auto, thick]
        
        \node [block] (Sample) {Sample Generation};
        \node [block, below right of=Sample, xshift=4em] (Estimate) {Model Fitting /\\ Return Estimation};
        \node [block, below left of=Estimate, xshift=-4em] (Improve) {Policy \\ Improvement};
        
        \path [line] (Sample) to [out=-30,in=150] node[midway, above right, xshift=-0.5em, yshift=0em]{} (Estimate);
        \path [line] (Estimate) to [out=-150,in=30] node[midway, below right, xshift=-0.5em, yshift=0em]{} (Improve);
        \path [line] (Improve) to [out=90,in=-90] node[midway, below left, xshift=2em, yshift=1em]{} (Sample);
    \end{tikzpicture}
    \caption{High-level process of RL algorithms: The algorithms start with the generation of samples through interaction with the environment (1). The samples are then used to estimate the return or fit a model (2). Based on the information gained, the policy is improved (3). This process continues in a loop until the policy converges towards an optimal policy or a predefined stopping criterion is met.}
    \label{fig:RL_process}
\end{figure}

\subsection{Deep Reinforcement Learning Algorithms}


\subsection{Model-based and Model-free learning}
In the literature of reinforcement learning (RL), the learning approaches can be classified primarily into two categories: \vocab{Model-based learning} and \vocab{Model-free learning} \cite{RL3}. In model-based learning approaches, the agent's strategy is to learn a model of the environment before deriving a policy. The agent creates an internal representation of the environment, encapsulating its rules and dynamics. As we have seen previously, a model of an environment in RL is most commonly defined as:
\[
P_{ss'}^{a} = \PP(s_{t+1}=s' \mid s_t=s, a_t=a)
\]
\[R_{ss'}^{a} = \mathbb{E}[r_{t+1} \mid s_t=s, a_t=a, s_{t+1}=s']
\]

Where $P_{ss'}^{a}$ is the transition probability from state $s$ to $s'$ under action $a$, and $R_{ss'}^{a}$ is the expected immediate reward when transitioning from $s$ to $s'$ with action $a$.

Once the model is well established, the agent utilizes it as a guide to derive and refine its policy \cite{RL2}. Although this approach might be more computationally demanding, it often results in a more robust and flexible policy, since the agent can exploit its comprehension of the environment to plan and adapt its actions across various situations \cite{RL}.

On the other hand, model-free learning approaches involve acquiring an action policy directly, without developing a detailed understanding of the underlying dynamics of the environment. In this case, the learning agent interacts with the environment based on the rewards it receives, instead of constructing a comprehensive model of how the environment operates.

An instance of model-free methods is the Q-learning algorithm. It learns an action-value function $Q(s,a)$, that gives the expected utility of taking a certain action $a$ in a certain state $s$, defined as \cite{RL3}:

\[
Q(s,a) = \mathbb{E}[r_t + \gamma Q(s_{t+1}, a_{t+1}) \mid s_t=s, a_t=a]
\]

The agent uses the Q-values to make decisions, but without explicitly constructing a model of the environment's dynamics. This property makes model-free methods less computationally expensive compared to their model-based counterparts, albeit at the cost of having less potential to adapt to changes in the environment.



\subsection{On-policy and Off-Policy Methods}
The distinction between ``on-policy" and ``off-policy" methods lies in the relationship between the policy that the agent follows while exploring the environment (behaviour policy) and the policy that it learns about (target policy). 
The fundamental distinction lies in the relationship between the policy that the agent follows while exploring the environment (behaviour policy) and the policy it learns about (target policy). 

In on-policy methods, the behaviour policy and the target policy are the same. The agent learns about and improves on the policy it uses to choose its actions. Any exploration must be part of this policy. In contrast, off-policy methods allows the agent to follow one policy (behaviour policy) while learning about a different, potentially optimal policy (target policy). The agent can explore more freely because it can take exploratory actions without directly impacting the quality of policy its learning about. 
`` On-policy methods attempt to evaluate or improve the policy that it used to make decisions, where as off-policy methods evaluate or improve a policy different from that used to generate the data" (Sutton and Barto, 2018).


\begin{itemize}
    \item \textbf{On-policy methods:} 
    These are the methods where the policy that's being improved upon is the same policy that's used to make decisions during interaction with the environment. In other words, the agent learns about and improves the policy that it is currently following. SARSA is a common example of an on policy method. For exploration, it might use an $\eps-$greedy strategy, but the important part is that the policy it learns about ($Q-$values it updates) is based on the same $\eps-$greedy strategy. 

    \item \textbf{Off-policy methods:} 
    These are methods where the policy that's being improved is different from the policy that's used to make decisions during interaction with the environment.
    The agent follows one policy (the behaviour policy), while learning about a different one (target policy). 
    For example, Q-learning is an off-policy method. The agent can use any exploratory strategy (like $\eps-$greedy), but the Q-values it learns are based on the assumption of a greedy policy (always choosing the action with the highest Q-value). 
\end{itemize}
 The benefit of off-policy methods is that they allow you to learn an optimal policy while following exploratory policy for better learning. In contrast, on-policy methods might converge faster because they are always learning about the policy they are following, but they need to balance exploration and exploitation with the policy they are learning about. 



