\section{Tabular Algorithms}\label{Algo}



Thus far we have been under the assumption that we have access to a model of the Markov Decision Process (MDP) environment, which includes transition states defined by $\mathcal{S}$ and the probabilities of the next state and reward given the current state and action\footnote{By model we mean the transition states define by $\SP$, probabilities of next state and reward, given current state and reward}.
However, in real-world scenarios, we often lack access to an MDP model. 
When we don't have a model, we don't have access to the environment's dynamics such as transition probabilities, $\PP(S_{t+1} \mid S_t , A_t)$, or rewards functions $r(s_{t+1}, a_t, s_t)$ . Consequently, we cannot predict the next state and the immediate reward you will receive when taking a specific action in a given state. 
Instead, the agent must interact with the actual MDP environment to learn about it. Interacting with the environment doesn't provide transition probabilities; it simply presents a new state and reward when an action is taken in a specific state. The environment supplies individual experiences of the next state and reward, rather than the explicit probabilities of occurrence for the next states and rewards. This raises a pertinent question: is it possible to compute the optimal value function or optimal policy without access to a model?



\subsection{Dynamic Programing}
Dynamic Programming (DP) is a mathematical approach used to solve complex problems by breaking them down into simpler subproblems. It is particularly effective when the problem has overlapping subproblems and optimal substructure. In the context of Reinforcement Learning, the two main DP algorithms are \vocab{Policy Iteration} and \vocab{Value Iteration}. These algorithms involve the evaluation and improvement of policies in order to find the optimal policy that maximizes the expected cumulative reward.


The mathematical formulation for these algorithms involves the Bellman equations. For a given policy $\pi$, the value function $V^\pi$ is defined as the expected return from each state, and satisfies the following Bellman equation for policy evaluation:

\begin{equation}
V^\pi(s) = \sum_{a} \pi(s,a) \sum_{s',r} \PP(s',r|s,a) \left[r + \gamma V^\pi(s')\right], \quad \forall s \in \mathcal{S}
\end{equation}
where $\PP$ represents the state transition probability, and $\gamma$ is the discount factor that determines the present value of future rewards.

In Policy Iteration, we alternately evaluate the current policy using the Bellman equation for policy evaluation, and improve the policy by making it greedy with respect to the current value function:

\begin{equation}
\pi'(s) = \arg \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V^\pi(s')\right], \quad \forall s \in \mathcal{S}
\end{equation}
Value Iteration combines the policy evaluation and policy improvement steps into a single update:

\begin{equation}
V_{k+1}(s) = \max_{a} \sum_{s',r} p(s',r|s,a) \left[r + \gamma V_{k}(s')\right], \quad \forall s \in \mathcal{S}
\end{equation}
The iterations continue until the value function (or the policy in policy iteration) converges, and the final value function represents the maximum expected return from each state, under the optimal policy.

In practical scenarios, Dynamic Programming (DP) is not commonly used to solve Reinforcement Learning problems. This is primarily due to the following reasons:
\begin{itemize}
    \item It's challenging to extend DP to handle continuous actions and states. DP methods typically require discretized states and actions, which can be impractical or infeasible for many real-world problems.
    \item DP methods necessitate access to the environment's model, specifically, the transition probabilities $P_{ss'}^a$. In reality, such detailed information about the environment is rarely available. Instead, we can only obtain samples from the environment by interacting with it and collecting experiences. These experiences can then be used to approximate the expectations via sampling-based methods, such as Temporal Difference (TD) learning methods.
\end{itemize}
However, there are advantages to using DP, including: DP methods provide mathematically precise solutions that can be formally expressed and analyzed. For smaller-scale problems, with a few thousand states or few tens to hundreds of actions, DP could be the most suitable method. It offers stability, simplicity, and straightforward convergence guarantees.
While DP isn't always practical for larger or continuous problems, the concepts it introduces are foundational to understanding more advanced reinforcement learning techniques we will be looking in the next section(s).

\subsection{Monte Carlo Learning}
Monte Carlo (MC) methods in reinforcement learning involve learning from complete sample returns. For a given policy, $\pi$, MC methods estimate the value-function $v_{\pi}(s)$ or $q_{\pi}(s,a)$  as the average of the returns observed in a number of episodes. 

The return $G_t$ at a time-step $t$ in an episode is defined as the cumulative discounted reward from that time-step onwards:

\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
\]

where $R_{t+i}$ is the reward after $i$ steps, and $\gamma$ is the discount factor. 

The estimate $V(s)$ for the value of a state $s$ is obtained by averaging the returns following all visits to $s$. We maintain a sum $S(s)$ and a count $N(s)$ for each state $s$:

\[
S(s) = \sum_{t \in \text{visits}(s)} G_t, \quad N(s) = \text{number of visits to } s
\]

Finally, the value of state $s$ is estimated as:

\[
v_{\pi}(s) \approx V(s) = \frac{S(s)}{N(s)}
\]

One major advantage of Monte Carlo methods is that they do not require a model of the environment; they learn directly from raw episodes of experience. These methods are both conceptually simple and easy to implement.


\begin{algorithm}
\caption{Monte Carlo ES (Exploring Starts) for estimating $\pi \approx \pi^*$}\label{MCES}
\begin{algorithmic}[1]
\Procedure{MonteCarloES}{$\mathcal{S}, \mathcal{A}, \gamma$}
\State Initialize $\pi(s) \in \mathcal{A}(s)$ arbitrarily for all $s \in \mathcal{S}$
\State Initialize $Q(s, a)$ arbitrarily for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$
\State Initialize Returns($s, a$) as an empty list, for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$

\While{True} \Comment{Loop forever (for each episode)}
    \State Choose $S_0 \in \mathcal{S}$, $A_0 \in \mathcal{A}(S_0)$ randomly such that all pairs have probability $> 0$
    \State Generate an episode from $S_0, A_0$, following $\pi: S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T$
    \State $G \gets 0$
    \For{each step of episode, $t = T-1,T-2,...,0$}
        \State $G \gets \gamma G + R_{t+1}$
        \If{the pair $S_t, A_t$ is unique in $S_0, A_0, S_1, A_1, \ldots, S_{t-1}, A_{t-1}$}
            \State Append $G$ to Returns($S_t, A_t$)
            \State $Q(S_t, A_t) \gets$ average(Returns($S_t, A_t$))
            \State $\pi(S_t)\gets \max_a Q(S_t, a)$
        \EndIf
    \EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}




\subsection{Temporal difference learning}

The Temporal-Difference (TD) learning method is prevalently employed in reinforcement learning applications \cite{RL3}.. TD learning is a method for estimating the expected cumulative future reward, or ``value," of each state in an environment. This value is updated at each time increment based on the difference between the predicted value of a state and the observed value after taking an action and transitioning to a new state \cite{RL}, \cite{RL2}.
TD learning leverages a hybrid approach that combines Monte Carlo's model-free experience-based value estimation with the benefits of dynamic programming's capability to calculate values utilizing present estimates alone \cite{sutton1988b}. 
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma \cdot V(S_{t+1} - V(S_t))
\end{equation}
\begin{itemize}
    \item We refer to $R_{t+1} + \gamma \cdot V(S_{t+1})$ as the \textbf{TD target}
    \item We refer to $\delta_{t} = R_{t+1} + \gamma \cdot V(S_{t+1}) - V(S_{t})$ as  \textbf{TD error}
\end{itemize}
The TD target is a (biased) estimate of $G_t$ \cite{RL}. The TD control algorithm can update the Q-value function after each time step. 


\subsection{Generalized Policy Iteration (GPI)}
 Virtually every reinforcement learning (RL) control algorithm has its roots in the fundamental principle of Generalized Policy Iteration (GPI). 
 However, the exact implementation of GPI varies and differs from one algorithm to another. 
 Therefore, prior to delving into the specifics of RL Control algorithms, its crucial to thoroughly understand the overarching concept of GPI. 

The term Generalized Policy Iteration (GPI) refers to the approach of interweaving Policy-Evaluation and Policy-Improvement procedures. 
GPI assess the Value function for a given policy, $\pi$, using \textit{any} Policy Evaluation method, and subsequently improves the policy using \textit{any} Policy Improvement method.

That is, the policy consistently improves itself in relation to the Value Function, while the Value Function moves towards the Value Function corresponding to the policy.
If both the evaluation process and improvement process reach an equilibrium (i.e., no further changes transpire), then the resulting policy and Value Function must be optimal. 

The value function reaches stability once it aligns with the current policy, i.e., when the policy evaluation step is complete. The policy, on the other hand, stabilizes when it becomes greedy with respect to the current value function, that is, during the policy improvement step. Both processes reach an equilibrium when a policy that is greedy with respect to its own evaluation function is found. This is the optimal policy, where any further changes neither improve the value function nor the policy itself.

Within the Generalized Policy Iteration framework, both an \textit{approximate policy} and \textit{approximate value function} are maintained. The Value Function is continually modified to more closely approximate the true Value Function of the current policy, while the policy is perpetually improved in relation to the existing Value Function.  While these two modifications are somewhat counteractive -- each sets a dynamic target for the other --  their combined effect drives both the policy and Value Function towards optimally. 
Policy evaluation is performed through experience from many episodes, with the approximate value function converging asymptotically to the true function.


One of the most important features of the GPI framework is its ability to accommodate 'partial' policy improvement. The term 'partial' here encompasses a broad range of computations, any of which contribute either to a full policy evaluation or towards policy improvement. Consequently, GPI offers flexibility to transition between policy evaluation and policy improvement without the need for completely exhaustive policy evaluation or complete policy improvement. For instance, we do not have to wait for policy evaluation computations to fully converge before proceeding with policy improvement.


The policy improvement theorem provides a guarantee that each successive policy $\pi_{k+1}$ is uniformly superior to, or at least as effective as, its predecessor $\pi_{k}$. In situations where $\pi_{k+1}$ is equivalent  to $\pi_{k}$, they are both classified as optimal policies. This implies that the overall process will ultimately converge to an optimal policy and corresponding optimal value function. 

In the context of Monte Carlo policy iteration, it is intuitively appealing to alternate between evaluation and improvement on a per-episode basis. Following each episode, the observed returns serve as inputs for policy evaluation, after which the policy undergoes refinement at all the states encountered during the episode.

\subsection{Why $Q-$function for Control?}

The $Q$-function serves as the an evaluative metric for state-action pairs $(s,a)$, under a specific policy $\pi$. Recall that the value of $(s,a)$ is the expected cummulative discounted reward obtained from taking action $a$ in state $s$, followed by adherence to policy $\pi$ in the subsequent course of actions.
In situations where an environmental model is not available, alongside determining the value of various states, an agent needs to also consider alternative moves and the expected consequences of making those moves. 
\note{
The $Q^{\pi}(s,a)$ function assigns a numerical value to each potential action, this value can be used to determine of the most optimal move to execute in a given state.}

One of the key advantages of $Q^{\pi}(s,a)$ is its provision of a direct strategy for agent action. 
Agents can compute $Q^{\pi}(s,a)$ for each feasible action $a \in A(s)$ within a given state $s$, and select the action the action with the highest value.
When $Q^{\pi}(s,a)$ is optimal, it gives the maximum expected value from taking action $a$ in state $s$, representing the best potential performance if the agent were to operate optimally in all future states. Hence, a knowing $Q(s,a)$ is automatically yields an optimal policy.

On the other hand, in order to utilize $V^{\pi}(s)$ for action selection, the agent needs to experiment with each of the available actions $a \in A(s)$ in a state $s$, observe the consequent state transition to $s'$, and keep track the resulting reward. Only then can an agent can act optimally by choosing the action leading to the highest expected cumulative discounted reward,  $\EE[r + V^{\pi}(s')]$. 
However, in situations where state transitions are stochastic, that is, where taking action $a$ in state $s$ can result in varying next states $s'$, the agent might have to replicate this process repeatedly to acquire a reliable estimate of the expected value from a specific action. This one-step look-ahead requirement for $V^{\pi}$ often poses a challenge in RL Control problems. $Q^{\pi}(s,a)$ circumvents this issue by directly learning the value of $(s,a)$, thereby storing the one-step look-ahead for each action $a$ in every state $s$. Therefore, for Control algorithms, which select actions based on a learned value function generally approximate the approximate $Q^{\pi}(s,a)$.

\subsection{Eligibility Traces}