\section{Background}
\subsection{Markov Decision Process}
A Markov Decision Process (MDP) is a mathematical framework used in reinforcement learning to model decision-making in situations where an agent interacts with an environment that has probabilistic state transitions and rewards \cite{Markov}. 

The motivation for using MDPs in reinforcement learning is to provide a structured representation of the problem, facilitating the development of algorithms that enable agents to find optimal policies for maximizing cumulative rewards over time \cite{RL2}.
\begin{definition}[Markov Decision Process]
    MDP is defined by 4-tuple ($\cS$, $\SA$, $\SP_a$, $\SR$):
    \begin{enumerate}
        \item a set of states $\cS$, possibly infinite
        \item a set of actions $\SA$, possibly infinite
        \item  transition probability $\SP_a(s', r \mid s, a) = \PP [S_{t+1} = s', R_t = r_t| S_{t} =s_t ,A_{t} = a_t]$
        \item a reward $R_{t+1} \in \SR \subseteq \RR$, and corresponding probability distribution $ \SR_a(s',s) = \PP[r' | s,a]$ over rewards
    \end{enumerate}
\end{definition}
 In the realm of Reinforcement Learning, the terms \textbf{state} and \textbf{observation} bear distinct meanings. A state $s$ is a comprehensive representation of the environment's current condition. By its very definition, it encapsulates all the requisite information about the world, leaving no detail concealed or unaccounted for. On the contrary, an \textbf{observation} $o$ offers a more limited view. It provides a partial portrayal of the state, potentially excluding certain aspects or details. In essence, while a state offers a complete picture, an observation might only present a snippet.
 
When an agent possesses the capability to discern the entirety of the environment's state, we term such an environment as \textbf{fully observed}. Conversely, in situations where the agent perceives only a subset or a partial snapshot of the environment's state, we categorize the environment as \textbf{partially observed}.


\note{
 There are instances where the symbol for states, $s$, is used in contexts where it would be technically more precise to utilize the symbol for observations, $o$. This ambiguity particularly surfaces when discussing the agent's decision-making process. Notationally, it's often conveyed that the agent's action is contingent on the state, symbolized as $s$. However, in a more practical scenario, the agent's action is indeed predicated on the observation, $o$, primarily because the agent doesn't typically possess complete access or knowledge of the true underlying state.}


In the domain of Deep Reinforcement Learning (Deep RL), the representation of both states and observations predominantly relies on real-valued vectors or higher-order tensor. This choice of representation facilitates the handling and processing of complex data types and structures. For illustrative purposes, consider a scenario where the environment provides a visual observation; this can be aptly represented using a tensor corresponding to the RGB values of each pixel. Similarly, when we are dealing with a robotic environment, the state might encompass information about the robot's kinematics, typically encapsulated through parameters such as joint angles and velocities.



The literature offers several definitions of MDPs, particularly regarding the reward function, which is often represented as $\SR(s)$, $\SR(s,a)$, or $\SR(s,a,s')$. While these expressions may differ, they share equivalent expressive power. For instance, a stochastic reward function can be emulated using a deterministic reward formulation by incorporating the reward into the state description.

In addition to the state, action, and reward definitions, some problem settings also consider the \textbf{initial state distribution}, represented as $\mu(s)$. This distribution defines the probability that the initial state $s_0$ is sampled from, playing a significant role in shaping the initial steps of the RL process.

In a \textit{finite} MDP, random variables $R_t$ and $S_t$ possess well-defined discrete probability distributions, which depend solely on the preceding state and action. For specific values $r \in \SR$ and $s' \in \cS$, there exists a probability of these values occurring at time $t$, given particular values of the preceding state and action. The transition probabilities from one time step to the next can be represented as a function $\SP: \cS \times \SR \times 
\cS \times \SA \to [0,1]$, which is an ordinary deterministic function\cite{RL}:

\begin{equation}
    \SP(s', r \mid s, a) = \mathbb{P}\{S_{t+1} = s', R_{t+1} = r \mid S_t = s, A_t = a\}
\end{equation}
Markov Decision Processes (MDPs) have been used to model a wide range of decision-making problems across various domains, some read-world examples include:
\begin{itemize}
    \item Robotics: In robotics, MDPs are employed for navigation and path planning tasks \cite{thrun2005}
    \item Finance: MDPs have been used to model and solve portfolio optimization problems \cite{buehler2019}
    \item Healthcare: MDPs have been employed in healthcare for personalized treatment recommendation system 
    \cite{komorowski2018}
\end{itemize}

\subsection{Episodic Reinforcement Learning}

In the episodic setting of reinforcement learning, the agent's experience is broken up into a series of episodes -- sequences with finite  number of states, actions and rewards. Episodic reinforcement learning in the fully-observed setting is defined by the following process. Each episode begins by sampling an initial state of the environment, $s_0$, from distribution $\mu(s_0)$. Each time step $t = 0,1,2, \dots,$ the agent chooses an action $a_t$, sampled from distribution $\pi(a_t, s_t)$. Then the environment generates the next state and reward, according to some distribution $\SP(s_{t+1}, r_t \mid s_t, a_t)$. The episode ends when a terminal state $s_T$ is reached. 




\tikzset{%
   neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.1cm,
    execute at begin node=\color{black}$\vdots$
  },
}

% The command \DrawNeuronalNetwork has a list as argument, each entry is a
% layer. each entry has the form 
%  Layer name/number of nodes/color/missing node/label/symbolic number
% where
% * layer name is, well,  the name of the layer
% * number of nodes is the number of neurons in that layer (including the missing neuron)
% * color is the color of the layer
% * missing node denotes the index of the missing neuron
% * label denotes the label of the layer
% * symbolic number denotes the symbol that indicates how many neurons there are
% * node content
\newcommand{\DrawNeuronalNetwork}[2][]{
\xdef\Xmax{0}
\foreach \Layer/\X/\Col/\Miss/\Lab/\Count/\Content [count=\Y] in {#2}
{\pgfmathsetmacro{\Xmax}{max(\X,\Xmax)}
 \xdef\Xmax{\Xmax}
 \xdef\Ymax{\Y}
}
\foreach \Layer/\X/\Col/\Miss/\Lab/\Count/\Content [count=\Y] in {#2}
{\node[anchor=south] at ({2*\Y},{\Xmax/2-0.8}) {\Layer};
 \foreach \m in {1,...,\X}
 {
  \ifnum\m=\Miss
   \node [neuron missing] (neuron-\Y-\m) at ({2*\Y},{\X/2-\m}) {};
  \else
   \node [circle,fill=\Col!50,minimum size=0.5cm] (neuron-\Y-\m) at 
  ({2*\Y},{\X/2-\m}) {\Content};
 \ifnum\Y=1
  \else
   \pgfmathtruncatemacro{\LastY}{\Y-1}
   \foreach \Z in {1,...,\LastX}
   {
    \ifnum\Z=\LastMiss
    \else
     \draw[->] (neuron-\LastY-\Z) -- (neuron-\Y-\m);
    \fi
    }
  \fi
 \fi
 \ifnum\Y=1
  \ifnum\m=\X
   \draw [overlay] (neuron-\Y-\m) -- (state);
  \else
   \ifnum\m=\Miss
   \else
    \draw [overlay] (neuron-\Y-\m) -- (state);
   \fi
  \fi
 \else
 \fi     
 }
 \xdef\LastMiss{\Miss}
 \xdef\LastX{\X}
}
}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=1.5cm, y=1.2cm,
    >=stealth,font=\small, nodes={align=center} ]
     \begin{scope}[local bounding box=T]
      \path  node[draw,minimum width=6em,minimum height=2em,yshift=-2.2em] (state) {State};
      \begin{scope}[local bounding box=NN]
       \DrawNeuronalNetwork{Input Layer/5/red/4///,
         Hidden Layer/5/green/4//11/,
         Output Layer/4/blue/3//11/}
      \end{scope}
      \path (NN.south) node[below]{Estimated parameter\\ $\theta$};
      \path(NN.east) -- node[above]{Policy\\ $\pi_{\theta}(a|s)$}++ (4em,0);
     \end{scope} 
     \node[fit=(T),label={[anchor=north west]north west:Agent},inner sep=1em,draw,]
      (TF){};
     \node[below=3em of TF,draw,inner sep=1em, yshift= 2em] (Env) {Environment};
     \draw[<-] (TF.180) -- ++ (-2em,0) |- (Env.180) node[pos=0.45,left]{$(s_{t+1}, r_t)$};
     \draw[->] (NN.east) -- ++ (7em,0)node[right]{$a_t$} |- (Env);
    \end{tikzpicture}
\end{figure}



\subsection{Policy and Return}
In reinforcement learning algorithms, assessing the value of an agent being in a particular state is crucial. 
Almost all reinforcement learning algorithms involve estimates of how good is it for the agent to be in a given state \cite{RL}.
However, defining ``how good" a state is can be ambiguous. In RL, the value of a state is determined by anticipated future rewards, or more precisely, in terms of expected rewards.

Of course the rewards the agent can expect to receive in the future depends on the actions it will take. Accordingly, the values of states (i.e., the expected reward agent will receive from that state on wards) are defined with respect to a \vocab{policy}.

In reinforcement learning, a policy is a strategy or set of rules that guides an agent's decision-making process, determining which action to take in a given state to maximize cumulative rewards over time. Policies are just functions that take as input a state, $S_t$, and output a probability distribution of actions.
For a given state $s \in S$ and action $a \in A$, the policy is defined as \cite{mohri2018}:
\begin{definition}[Policy]
    A policy is a mapping $\pi: S \to \Delta(A),$ where $\Delta(A)$ is the set of probability distributions over the action space $A$. A policy is deterministic if for any $s \in S$, there exists a unique $a \in A$ such that $\pi(a|s) = 1$. In this case, we can identify $\pi: S \to A$.
    \footnote{The given definition pertains to a stationary policy, as the action distribution does not rely on time. Generally, a non-stationary policy can be characterized as a sequence of mappings $\pi_t : S \rightarrow \Delta(A)$, indexed by $t$. Notably, in scenarios with a finite horizon, employing a non-stationary policy is often essential for reward optimization}
\end{definition}







The objective in RL is to learn a policy, more precisely, the agent's objective is to find a policy that \textit{maximizes its expected (reward) return}. The discounted return is defined as \cite{RL}:
\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2 } + \gamma^2 R_{t+3 } + \cdots = \gamma^k \sum_{k=0}^{\infty} R_{t+k+1}
\end{equation}
Where $\gamma \in [0,1]$ is the \textit{discount-factor}\footnote{ The discount factor $\gamma$ serves to model scenarios in which a future reward is considered less desirable than an immediate reward of the same magnitude.}.
The return is a scalar-value that essentially summarizes a (possibly) infinite sequence of immediate rewards. 
An important fact to note is that even though this is an infinite sum, if $\gamma < 1$, $G_t$ will be finite, as long as $\{R_k\}$ is bounded \cite{RL}. If we set the value of $\gamma$ to $0$, the agent will only consider immediate reward, and future rewards will be of no consequence. Conversely, as  $\gamma \to 1$, the agent values future rewards more strongly \cite{RL}. The return $G_t$ for a given time step $t$ can be expressed recursively as:
\begin{align}
    G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots  \nonumber \\
    &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \cdots) \nonumber \\
    &= R_{t+1} + \gamma G_{t+1} \label{return}
\end{align}
By breaking down the computation of expected returns into smaller, incremental steps, the recursive property enables efficient updating of value estimates during the learning process. This is particularly useful in temporal difference learning methods, such as Q-learning and SARSA, where the value function is updated incrementally after each transition. This recursive property allows RL algorithms to leverage current estimates of future returns, $G_{t+1}$, to improve the current value estimate $G_t$. This process, known as bootstrapping, helps algorithms converge faster by taking advantage of information already available, rather than waiting to accumulate the complete return through the end of an episode. We will delve deeper into this idea in Section \ref{Algo}, where we explore Temporal Difference learning and Q-learning.


In deep RL, we deal with parameterized policies: policies whose outputs are computable functions that depend on a set of parameters (e.g., weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. 

\subsection{Goal of Reinforcement Learning}

In reinforcement learning, we define the agent's experience as a trajectory, which includes a sequence of states, actions, and rewards. At each time step $t$, the agent is in a state $s_t$ and takes action $a_t$ according to policy $\pi(a_t \mid s_t)$, and transition probabilities to a new state $s_{t+1}$ according to environment's transition probabilities $p(s_{t+1} \mid s_{t}, a_{t})$. A reward $r_t$ is also received.
\begin{align*}
    p_{\theta}(\boldsymbol{s}_1, \boldsymbol{a}_1, \cdots ,\boldsymbol{s}_T, \boldsymbol{a}_T) = p(\boldsymbol{s}_1) \prod_{t=1}^{T} \pi_{\theta}(\boldsymbol{a}_{t} \mid \boldsymbol{s}_t) p(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_t)
\end{align*}

\note{
A \vocab{trajectory} $\tau$, also called an episode, is sequence of states, actions and rewards from the start to the end of an episode, $\tau \equiv (\boldsymbol{s}_1, \boldsymbol{a}_1, \cdots ,\boldsymbol{s}_T, \boldsymbol{a}_T)$. We will use the notation $p_{\theta}(\tau)$ to denote the probability of the entire trajectory $\tau$ under policy parameters $\theta$, and let $R(\tau)$ denote the total reward of the trajectory.}


The distribution over trajectories under policy $\pi_\theta$ is given by the product of an initial state distribution, the policy probabilities for the actions, and the environment's transition probabilities for the state. 


\begin{align*}\label{RL-Objective}
    \theta^{\star} = \arg \max_{\theta} \EE_{\tau \sim p_{\theta}(\tau)} \left[ 
    \sum_{t} r(\boldsymbol{s}_t, \boldsymbol{a}_t)
    \right]
\end{align*}

The objective in reinforcement learning is to find the policy parameters $\theta$ of a policy $\pi{\theta}(a_t \mid s_t)$, which is a conditional distribution over actions $a_t$ conditioned states $s_t$, with respect to the expectation of a reward function $r(s_t, a_t)$. 

Our goal is to maximize the expected return, where the return is defined as the sum of rewards over trajectory. The expectation here is taken over the distribution of trajectories. The objective reflects the goal of finding a policy that, on average, produces trajectories with high rewards as possible, taking into account the randomness in the policy, the environment's transitions, and the initial state distribution. 

\subsection{Value and State-Value Functions}

Now we can define the notion of ``value of a state" formally via \vocab{value functions}. Value functions in RL are tools for estimating the long-term `value' or desirability of being in a given state. They allow the agent to access how good it is to be in a certain state, based on the expected cumulative reward obtained from that point onwards. There are two types of value functions commonly used in reinforcement learning: $V^{\pi}(s)$ and $Q^{\pi}(s,a)$, we define them formally below \cite{RL}, \cite{mohri2018}:
\begin{definition}[Value function]
    The value $V^{\pi}(s)$ for a fixed policy $\pi$ at a given state $s \in S$ represents the expected reward obtained when initiating at state $s$ and adhering to policy $\pi:$
    \begin{itemize}
        \item $V^{\pi}(s) = \underset{\pi_{theta}}{\EE} [G_t \mid S_t = s]$for all $s \in S$ \cite{RL}
        \item finite horizon: $V^{\pi}(s) = \underset{\pi_{\theta}}{\EE} \left[ \sum_{t=0}^T r(s_t,a_t) \mid S_t =s
        \right]$ \cite{mohri2018}
        \item infinite discounted horizon: $V^{\pi}(S_t = s) = \underset{\pi_{\theta}}{\EE} \left[ \sum_{t=0}^{+ \infty} \gamma^t r(s_t,a_t) \mid S_t =s
        \right]$ \cite{mohri2018}     \footnote{Technically the notation should be: $\EE_{a_t \sim \pi(s_t)} \EE_{s_t}$ since
    we are not just taking the expectation over the random selection of an action $a_t$ according to the policy distribution $\pi(a_t \mid s_t)$, but we are also taking the expectation over the states $s_t$ reached and the corresponding reward values $r(s_t, a_t)$. However, in RL literature/textbooks the randomization with respect to the next state and reward function doesn't seem to be  explicitly mentioned (to simplify notation perhaps)} 
    \end{itemize}
\end{definition}
An intuitive way of understanding Value Function is that it tells us how much ``accumulated future reward'' we expect to obtain from a given state.

Starting from a state $s \in S$, to maximize reward, an agent naturally seeks a policy $\pi$ with the largest value $V_{\pi}(s)$. A remarkable result for finite MDPs in the finite horizon case, there exists an \textit{optimal policy} for all starting states $s \in S$. Formally, a policy $\pi^*$ is optimal if for any policy $\pi$ and any state $s \in S$, $V^{\pi^*}(s) \geq  V^{\pi}(s)$. 
We can prove that this optimal policy $\pi^*$ is deterministic \cite{mohri2018}, and this proof can be found in Mohri  et al. (2018). 
\\\
\\
\textbf{Action-Value Function} ($Q^{\pi}$): Represents the expected cumulative reward an agent can obtain by taking an action $a$ in a given state $s$ and following a certain policy $\pi$ thereafter. 

\begin{definition}[State-action value function]
    The state-action value function $Q$ associated to a policy $\pi$ is defined for all $(s,a) \in S \times A$ as the expected return for taking action $a \in A$ at state $s \in S$ and then following policy $\pi$:
    \begin{align*}
    Q^{\pi}(s_t,a_t) &=  \underset{\pi_{\theta}}{\EE} [G_t \mid S_t =s, A_t = a]  = \underset{\pi}{\EE}
    \left [
    \sum_{k =0}^{+ \infty} \gamma^t r(s_{k}, a_k) \mid S_t = s, A_t = a \right ]
    \end{align*}
\end{definition}
$$
V^{\pi}(s_t) = \EE_{a_t \sim \pi(a_t \mid s_t)}[Q^{\pi}(s_t, a_t)]
$$
The motivation behind using value functions in RL is to guide the agents decision-making process. By estimating the values of states or state-action pairs, the agent can choose actions that lead to states with higher-long term rewards. We will see in next section that values serve as the basis for various RL algorithms, such as $Q-$learning and SARSA.

Value functions $V^{\pi}$ and $Q^{\pi}$ can be estimated through experience. For instance, if an agent follows policy $\pi$ and maintains an average for each state encountered, based on the actual returns following that state, these averages will converge to $V^{\pi}(s)$ for all $s \in S$ as the number of times the state is encountered approaches infinity.
By keeping separate averages for each action taken in each state, these averages will converge to the action-values $Q^{\pi}(s,a)$. Such estimation methods are referred to as \textbf{Monte Carlo methods} because they involve averaging over numerous samples of actual returns.
\subsection{The Bellman Equations}
Value functions in reinforcement learning exhibit recursive relationships, like the one established for the return in equation (\ref{return}). For every policy $\pi$ and state $s$, this recursive relationship exists that links the value of $s$ to the values of its potential subsequent states \cite{RL}:
\begin{align*}
    V^{\pi}(s) &= \EE_{\pi}[G_t \mid S_t =s] \\
    &= \EE_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t =s] \\
    &= \sum_{a \in A} \pi(a\mid s) \sum_{s'} \sum_{r} \PP(s',r \mid s,a) [r + \gamma \term{$\EE_{\pi} (G_{t+1} \mid S_{t+1} = s')$} ] \\
    &=\sum_{a \in A} \pi(a\mid s) \sum_{s'} \sum_{r} \PP(s',r \mid s,a) [r + \gamma \term{$V^{\pi}(s')$} ], \quad \forall s \in S
\end{align*}
The equation represents a recursive formulation of the value function, and this is known as the \vocab{Bellman Expectation Equation} \cite{RL}, \cite{RL2}, \cite{bellman1958}.
The Bellman equation is a fundamental equation in reinforcement learning and in dynamic programming. 
\note{In simple terms, the Bellman Equation decomposes the value function into immediate reward from taking an action in the current state plus the discounted expected value of future states, while taking into account all future possible actions.}
The Bellman equation tells us that the value of any state must be equal to the expected reward received from being in that state plus the (discounted) value of the expected next state.
Another more compact way to express the Bellman equation commonly found in the literature is: 
\begin{equation}\label{Bell1}
    V^{\pi}(s) = \sum_{a \in \SA(s)} \pi(a|s) \cdot \sum_{s'} \SP_{s,s'}^{a} [r(s,a) + \gamma V^{\pi}(s')]
\end{equation}
The equation states that the value of a given state, $V^{\pi}(s)$ under $\pi$, is equal to the immediate reward $r(s,a)$ plus discounted value of the next state $\gamma V^{\pi}(s')$. Recall that $V^{\pi}(s')$ is also an expectation, and therefore, it is considering all possible actions and transitions to the next state, weighted by their probabilities $\SP_{s,s'}^{a} = \PP(s' \mid s,a)$.
\\
Bellman Equation for Action-Value Function can be expressed in the same way:
\begin{align}
    Q^{\pi}(s,a) &= \SP_{s,s'}^{a} [ r(s,a) + \gamma \overbrace{\sum_{a'} \pi(a'|s') Q^{\pi}(s',a')}^{V^{\pi}(s)}] \\
    &= \SP_{s,s'}^{a} [ r(s,a) + \gamma V^{\pi}(s)]
\end{align}
Since, 
$$
V^{\pi}(s) = \sum_{a \in \SA(s)} \pi(a \mid s) Q^{\pi}(s,a)
$$
\subsection{Finding Optimal Policies}
Value functions allow us to create a hierarchical structure among policies, where a policy $\pi$ is deemed superior or equal to another policy $\pi'$ if it yields an expected return that is greater than or equal to that of $\pi'$ across all states \cite{RL2}. Thus, $\pi \geq \pi'$ if and only if the condition $V_\pi(s) \geq V_{\pi'}(s)$ holds true for every state $s \in S$ \cite{mohri2018}. For finite MDPs, there is always at least one policy that is better than or equal to all other policies \cite{RL}. This is what we call the \textit{optimal policy}. Although there might be more than one, we denote all the optimal policies by $\pi^*$. The optimal policies share the same state-value function and the action-value functions which we denote as $V^{*}$ and $Q^{*}$\cite{RL2}.
\begin{align*}
    V^{*}(s) = \underset{\pi}{\text{max }} V^{\pi}(s), \quad  \forall s \in S, a \in A \\ 
    Q^{*}(s) = \underset{\pi}{\text{max }} Q^{\pi}(s), \quad  \forall s \in S, \forall a \in A
\end{align*}
\note{
A useful fact which will be leveraging is that we can express $Q^*$ in terms of $V^*$:
\begin{align*}
    Q^*(s,a) &= \EE [R_{t+1} + \gamma V^*(S_{t+1}) \mid S_t = s, A_t =a ] \\ 
    &= \EE [R_{t+1}]  + \gamma \sum_{ s' \in \cS} \PP[S_{t+1} =s'|S_t = s, A_t = a] V^*(S_{t+1})   
\end{align*}
Note that $R_{t+1}$ is a random varible since we don't know what future state $s'$ we will end up in, therefore, 
we are taking the expectation $\EE [R_{t+1}]$ over the dynamics of transition probabilities of the next state $s'$ we reach.}
Recall that all value functions $v(s)$, must satisfy the recursive/self-consistency property established in (\ref{Bell1}). This mean we can express $V^*$ in a similar fashion. We write $V^*$ without  reference to any specific policy because it is the optimal value function. We follow a similar derivation which can be found in \cite{RL}, \cite{Markov}, \cite{RL2}, \cite{mohri2018}:
\begin{align}
    V^*(s) &= \underset{a \in A}{\text{max }} Q^{\pi}(s), \quad  \forall s \in S  \nonumber \\   
    &= \max_{a} \mathbb{E}_{\pi^*}[G_t \mid S_t = s, A_t = a] \nonumber \\
    &= \max_{a} \mathbb{E}_{\pi^*}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \nonumber \\
    &= \max_{a}  \mathbb{E}_{\pi^*}[R_{t+1} + \gamma V^*(S_{t+1}) \mid S_t = s, A_t = a] \nonumber \\
    &= \max_{a} \sum_{s'} \sum_{r} \PP(s',r \mid s,a) [r + \gamma V^{*}(s') ], \quad \forall s \in S 
\end{align}
This is known as the \vocab{Bellman Optimality Equation}. The equation expresses that ``the value of  state under \textit{optimal policy} must be equal to the expected for the best action from that state" \cite{RL}, and
is used to compute the optimal value function which is usually denoted as $V^*(s)$ in literature.
The Bellman optimally equation for $Q^*(s,a)$ is:
\begin{align*}
    Q^{*}(s,a) &= \EE \left [R_{t+1}
    + \gamma \underset{a}{\text{max}}Q^*(S_{t+1},a')  \mid S_{t} =s, A_t = a \right] \\
    &= \sum_{s',r}\PP(s',r \mid s,a)
    \left [r + \gamma \underset{a}{\text{max}}Q^*(S_{t+1},a') 
    \right]
\end{align*}
\textbf{Why are $Q^*$ and $V^*$ important?}
\begin{itemize}
    \item $V^*(s)$: When we have $V^*$, determining the optimal policy becomes easy. The beauty of $V^*$ lies in its ability to transform the greedy policy into the optimal policy in terms of long-term rewards \cite{RL}, as it takes into account the reward consequences of all possible future behaviors. Through $V^*$, the optimal expected long-term return is converted into a value that is readily and locally accessible for each state, allowing for more efficient decision-making \cite{RL}.
     
    \item $Q^*(s,a)$: With $Q^*$ at hand, the agent can bypass one-step ahead search. For any state $s \in S$, it can directly identify the action(s) that maximizes $Q^*(s,a)$. The action-value function serves as a cache for the results of one-step-ahead searches \cite{RL}, offering the expected long-term return as a value that is both locally and instantly accessible for each state-action pair.
\end{itemize}

