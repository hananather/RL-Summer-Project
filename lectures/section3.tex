\section{Deep Reinforcement Learning}


\subsection{Value Function Approximation}
Function approximation in reinforcement learning is a method used to estimate state-value functions and the policy functions.
The value function is not longer represented as a table but instead as a parameterized functional form with a weight vector $\mathbf{w}$ residing in $\RR^d$.

Commonly, we denote $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ to signify the approximate value of state $s$ given the weight vector $\mathbf{w}$. For instance, $\hat{v}$ could be a function computed by a neural network, with $\mathbf{w}$ serving as the weight vector for all connections. By fine-tuning these weights, the neural network progressively refines its approximation, moving ever closer to accurately representing the true value function. This continual fine-tuning allows the network to adapt and improve its predictions over time, enhancing its understanding of the environment and its dynamics.
Often, the number of weights is fewer than the number of states. As a consequence, when a single state undergoes an update, the modification proliferates from that state, influencing the values of other states.

Similar to supervised learning, value function approximation just boils down to finding the specific parameter vector, denoted as $\mathbf{w}$, which minimizes the loss between the real value function, $V_\pi(s)$, and its approximation, denoted as $\hat{V}(s; \boldsymbol{w})$. Generally use mean squared error and define the loss as
$$
J(\mathbf{w}) = \mathbb{E}_\pi[(V^\pi(s) - \hat{V}(s;  \mathbf{w}))^2]
$$
By minimizing this loss, we improve the accuracy of the approximated value function, bringing it closer to the true value function and thereby improving the performance of our model. We can  use gradient descent to find a local minimum:
$$
\Delta\mathbf{w} = -\frac{1}{2}\alpha \nabla_{\mathbf{w}} J(\mathbf{w})
$$
where,
\begin{align*}
\Delta_\mathbf{w} J(\mathbf{w}) &= \Delta_\mathbf{w} \mathbb{E}_\pi[(V^\pi(s) - \hat{V}(s;\mathbf{w}))^2] \\
&= \mathbb{E}_\pi[2(V^\pi(s) - \hat{V}(s;\mathbf{w})) \Delta_\mathbf{w} \hat{V}(s, \mathbf{w})]
\end{align*}


The majority of prediction methods can be viewed as updates to an estimated value function that nudges its value at a specific state toward a `backed-up value' or an update target. An individual update can be represented as $S_t \mapsto U_t$, where $S_t$ represents the state and $U_t$ denotes the update target towards which the estimated value of $S_t$ is shifted. For example, the Monte Carlo Update for value prediction, characterized as $S_t \to G_t$, and the Temporal Difference (TD(0)) update, depicted as $S_t \to R_{t+1} + \gamma \hat{v}(S_{t+1},w_{t})$.


Each update in reinforcement learning can be viewed as a training instance analogous to supervised learning. Function approximation methods in reinforcement learning receive examples of the input-output behavior of the function they aim to approximate. This allows us to use supervised learning techniques for value function approximation by treating each $s \to u$ update as a training instance. The resulting approximate function is then interpreted as an estimated value function.

\subsection{State-Value Function Approximation}
Analogous to value function approximation, we also use function approximation to estimate the state-value functions: $\hat{Q}_{\pi} (S_t, A_t)$. 
In control problems the parametric approximation of the action-value function is represented as $\hat{Q}(s,a,\mathbf{w} ) \approx q_{*}(s,a)$, where $\mathbf{w}  \in \RR^d$ is a finite dimensional weight vector. Instead of $S_t \mapsto U_t$, 
we now consider examples of the form $(S_t, A_t) \mapsto U_t$. 
\begin{itemize}
    \item $S_t$ represents the state at time $t$
    \item  $A_t$ is the action taken at time $t$.
\end{itemize}
Here, we're using $(S_t, A_t)$ as an example to predict $U_t$, which is our update target.
The update target $U_t$ can be any approximation of the action-value function such as the Monte Carlo return, $n$-step SARSA target, or Q-learning target. The key idea here is to use experiences to incrementally improve the approximation of the state-value, with the aim of improving the policy based on that approximation. Like in supervised learning the most convenient way to accomplish this is by minimizing the mean-squared error between the true \footnote{In practice we won't have access to the true action-value function}
action-value function $Q^\pi(s, a)$ and the approximate action-value function: 
$$
J(w) = \mathbb{E}_\pi[(Q^\pi(s, a) - \hat{Q}^\pi(s, a; w))^2]
$$
Use stochastic gradient descent to find a local minimum
\begin{align*}
\Delta(w) &= \alpha \nabla_w J(w) \\
&= \alpha \mathbb{E}[Q^\pi(s, a)  - \hat{Q}^\pi(s, a; w))\nabla_w \hat{Q}^\pi(s, a; w)]
\end{align*}
Just as in policy evaluation, the true state-action value function for a state is not known, and as such, we utilize a \textbf{target value} as a substitute.
\begin{itemize}
    \item \vocab{Monte Carlo Target}: 
    $$
    \Delta w = \alpha \left(G_t + \gamma \hat{Q}(s', a'; w) - \hat{Q}(s, a; w)\right) \nabla_w \hat{Q}(s, a; w)
    $$
    \item \vocab{SARSA Target}:
    $$
    \Delta w = \alpha \left(r + \gamma \hat{Q}(s', a'; w) - \hat{Q}(s, a; w)\right) \nabla_w \hat{Q}(s, a; w) 
    $$
    \item \vocab{Q-learning}:
    $$
    \Delta w = \alpha \left(r + \gamma \max_{a'} \hat{Q}(s', a'; w) - \hat{Q}(s, a; w)\right) \nabla_w \hat{Q}(s, a; w)
    $$
\end{itemize}
Theoretically, any supervised learning approach could be employed to estimate value functions. However, not all function approximation techniques are equally appropriate or efficacious when applied to reinforcement learning. Many statistical machine learning algorithms rely on a static training set derived from a stationary distribution. In contrast, reinforcement learning generates data sequentially as the agent interacts with its environment or a model of the environment. It is important that learning can occur in an online environment, and to facilitate this, supervised learning techniques that can approximate non-stationary target functions are required.
For instance, in control methods based on Generalized Policy Iteration, our objective is to learn $Q_{\pi}$ while $\pi$ undergoes changes. Even if the policy remains constant, the target values of training examples become non-stationary if they are generated by bootstrapping methods like Dynamic Programming or Temporal Difference learning. Thus, it is crucial to recognize that methods incapable of handling such non-stationary data are less suitable for reinforcement learning.
